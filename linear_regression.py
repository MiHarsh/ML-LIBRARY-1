# -*- coding: utf-8 -*-
"""Linear_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1959ZPCof4A3ZSA7f9MlNod2q7VYc5SwO
"""

#IMPORTING REQUIRED LIBRARIES
import numpy as np
import pandas as pd
import matplotlib
from matplotlib import pyplot as plt

#IMPORTING DATASET
df=pd.read_csv("/content/sample_data/mnist_train_small.csv")
df.head()

xa=(df.drop(["6"],axis=1)-128)/255  #APPLYING_NORMALISATION


ya=df["6"]


#WE CALCULATE THE COST FUNCTION BY (1/m)*Î£((h-y)**2)
#SO WE CALCULATE h{Hypothesis}

def hypothesis(theta, X,n):
    h = np.ones((X.shape[0],1))
    theta = theta.reshape(1,n+1)
    for i in range(0,X.shape[0]):
        h[i] = float(np.dot(theta, X[i]))
    h = h.reshape(X.shape[0])
    return h

#DEFINING_GRADIENT_DESCENT
def GradientDescent(theta, alpha, num_iters, h, X, y, n,lam):
    cost = np.ones(num_iters)
    for i in range(0,num_iters):
        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)
        for j in range(1,n+1):
            # theta[j] = theta[j] - (alpha/X.shape[0]) * sum((h-y) * X.transpose()[j])
            theta[j] = theta[j]*(1-alpha*lam/X.shape[0]) - (alpha/X.shape[0]) * sum((h-y) * X.transpose()[j])
        h = hypothesis(theta, X, n)
        # cost[i] = (1/X.shape[0]) * 0.5 * sum(np.square(h - y))
        cost[i] = (1/X.shape[0]) * 0.5 * (sum(np.square(h - y))+lam*sum(theta**2))
    theta = theta.reshape(1,n+1)
    return theta, cost

#DEFINING_LINEAR_REGRESSION
def linear_regression(X, y, alpha, num_iters,lam):
    n = X.shape[1]
    one_column = np.ones((X.shape[0],1))
    X = np.concatenate((one_column, X), axis = 1)
    # INITIALISING PARAMETER VECTOR....
    theta = np.zeros(n+1)
    # HYPOTHESIS CALCULATION....
    h = hypothesis(theta, X, n)
    # returning the optimized parameters by Gradient Descent...
    theta, cost = GradientDescent(theta,alpha,num_iters,h,X,y,n,lam)
    return theta, cost

theta, cost = linear_regression(xa, ya,0.001,6000,100)
cost

#PLOTTING_COST_FUNCTION
cost = list(cost)
n_iterations = [x for x in range(1,6001)]
plt.plot(n_iterations, cost)
plt.xlabel('No. of iterations')
plt.ylabel('Cost')

#IMPORTING TESTSET
dft=pd.read_csv("/content/sample_data/mnist_test.csv")
dft.head()

xt=(dft.drop(["7"],axis=1)-128)/255 #NORMALISATION ON TESTSET

yt=dft["7"]

n = xt.shape[1]
one_column = np.ones((xt.shape[0],1))
X = np.concatenate((one_column, xt), axis = 1)

#OUR_PREDICTION_BY_LINEAR_REGRESSION
y_pred=hypothesis(theta,X,n)

#DEFINING THRESHOLD FOR ESTIMATING THE PREDICTED_DATA
def threshold(arr,t):
  ypt=[]
  for i in arr:
    if (i-int(i))<=t:
      ypt.append(int(i))
    else:
      ypt.append(int(i)+1)
  return ypt

def accuracy(y_pred_threshold,y):
  count=0
  for i in range(X.shape[0]):
    if y_pred_threshold[i]==y[i]:
      count+=1
  return (count/X.shape[0])*100

#ACCURACY
def main(y_pred,y,t):
  y_pred_threshold=threshold(y_pred,t)
  return accuracy(y_pred_threshold,y)

main(y_pred,yt,0.4)
