# -*- coding: utf-8 -*-
"""Neural_Network_2_layers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QrsBI6woi8D4D3rtOHbU0hPfabP9sUCI
"""

#IMPORTING IMPORTANT LIBRARIES
import numpy as np
import pandas as pd
import matplotlib
from matplotlib import pyplot as plt

#IMPORTING DATASET
df=pd.read_csv("/content/sample_data/mnist_train_small.csv")
df.head()

df.describe()

x=df.drop(["6"],axis=1)

y=df["6"]

np.random.seed(123)

#NORMALISING 
def feature_normalize(X):
    n_features = X.shape[1]
    means = np.array([np.mean(X.iloc[i,:]) for i in range(n_features)])
    stddevs = np.array([np.std(X.iloc[i,:]) for i in range(n_features)])
    normalized = (X - means) / stddevs
    return normalized

X=feature_normalize(x)

X=np.column_stack((np.ones(len(X)),X))

#HYPOTHESIS WITH INBUILT SIGMOID FOR FORWARD PROPAGATION
def sigmoid(theta,arr):
  return 1/(1+np.exp(-arr@theta.T))

def compute_cost(h,y):
  return sum((-1/len(y))*(sum(y*np.log(h)+(1-y)*np.log(1-h))))

def ystack(arr):
  
  ys=np.tile(arr,(10,1)).T
  for i in range(10):
    for j in range(len(arr)):
      if ys[j][i]==[i]:
        ys[j][i]=1
      else:
        ys[j][i]=0
  return ys

#INITIALISING THETA1 AND THETA2(fOR 2 LAYER NEURAL NETWORK)
theta1=np.random.rand(20,X.shape[1])*0.01

theta2=np.random.rand(10,21)*0.01

#LOOP FOR UPDATING THE PARAMETERS
cost=[]
z=0
for i in range(100):
  l=0

  stack=ystack(y)
  for j in range(X.shape[0]//500):#USE 500 EXAMPLES AT A TIME AND UPDATE
    X1=X[l:l+500]
    Y1=stack[l:l+500]
    a2=sigmoid(theta1,X1)  #ACTIVATION OF SECOND LAYER(WITHOUT BIAS)
    a2bias=np.column_stack((np.ones(len(Y1)),a2))#WITH BIAS
    a3=sigmoid(theta2,a2bias) #HYPOTHESIS
    Del3=a3-Y1  #DELTA OF THE PREDICTION
    cost.append(compute_cost(a3,Y1))
    Del2=(((theta2.T)@Del3.T).T)*(a2bias*(1-a2bias))
    Grad2=(1/X1.shape[0])*(a2bias.T@Del3).T  #DEFINING GRADIENT1 AND GRADIENT 2
    Grad1=(1/X1.shape[0])*(X1.T@Del2[:,1:]).T
    #UPDATING PARAMETERS
    theta2=theta2-0.01*Grad2 
    theta1=theta1-0.01*Grad1
    l+=500  #INCREMENT IN VALUE OF L 
    z+=1

#Plotting Cost VS Iteration:
n_iterations = [x for x in range(1,z+1)]
plt.plot(n_iterations, cost)
plt.xlabel('No. of iterations')
plt.ylabel('Cost')

#IMPORTING TESTSET
dft=pd.read_csv("/content/sample_data/mnist_test.csv")

x_test=dft.drop(["7"],axis=1)

y_test=dft["7"]

X_test=feature_normalize(x_test)
X_test=np.column_stack((np.ones(len(X_test)),X_test))

#cALCULATION OF PREDICTION
def predict(X_test):
  a2_test=sigmoid(theta1,X_test)  
  a2bias_test=np.column_stack((np.ones(len(y_test)),a2_test))
  hyp=sigmoid(theta2,a2bias_test)
  return hyp.argmax(axis=1)

y_pred=predict(X_test)

def accuracy(y_pred,y_test):
  count=0
  for i in range(len(y_test)):
    if y_pred[i]==y_test[i]:
      count+=1
  return (count/len(y_test))*100

