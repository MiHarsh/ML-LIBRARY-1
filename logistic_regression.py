# -*- coding: utf-8 -*-
"""Logistic_regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z20F4Sf2jbiO6kb7QHQR0Ecd57NYRrlJ
"""

#IMPORTING_LIBRARIES
import numpy as np
import pandas as pd
import matplotlib
from matplotlib import pyplot as plt

#IMPORTING_MNIST_DATASET
df=pd.read_csv("/content/sample_data/mnist_train_small.csv")
df.head()

x=(df.drop(["6"],axis=1))/255  #APPLYING NORMALISATION
y=df["6"]
x.head()

#DEFINING HYPOTHESIS
def hypothesisn(theta,X,n):   #USED FOR PROBABILITY FINDING
  h=np.ones((X.shape[0],1))
  theta=theta.reshape(1,n+1)
  for i in range(0,X.shape[0]):
    h[i] = float(np.dot(theta, X[i]))
  h = h.reshape(X.shape[0])
  return h

#HYPOTHESIS WITH INBUILT SIGMOID
def hypothesis(theta,X,n):
  h=np.ones((X.shape[0],1))
  theta=theta.reshape(1,n+1)
  for i in range(0,X.shape[0]):
    h[i] = float(np.dot(theta, X[i]))
  h = h.reshape(X.shape[0])
  h=1/(1+np.exp(-h))
  return h

#DEFINING_GRADIENT_DESCENT
def GradientDescent(theta, alpha, num_iters, h, X, y, n,lam):
    cost = np.ones(num_iters)
    for i in range(0,num_iters):
        theta[0] = theta[0] - (alpha/X.shape[0]) * sum(h - y)
        for j in range(1,n+1):
          theta[j] = theta[j]*(1-alpha*lam/X.shape[0]) - (alpha/X.shape[0]) * sum((h-y) * X.transpose()[j])
        h = hypothesis(theta, X, n)
        cost[i] = (1/X.shape[0]) * 0.5 * (sum(np.square(h - y))+lam*sum(theta**2))
    theta = theta.reshape(1,n+1)
    return theta, cost

#DEFINING_LOGISTIC_REGRESSION
def logistic_regression(X, y, alpha, num_iters,lam):
    n = X.shape[1]
    one_column = np.ones((X.shape[0],1))
    X = np.concatenate((one_column, X), axis = 1)
    # INITIALISING PARAMETER VECTOR....
    theta = np.zeros(n+1)
    # HYPOTHESIS CALCULATION....
    h = hypothesis(theta, X, n)
    # returning the optimized parameters by Gradient Descent...
    theta, cost = GradientDescent(theta,alpha,num_iters,h,X,y,n,lam)
    return theta, cost

#MATRIX_Ycoll
ycoll=np.ones((x.shape[0],9))
for i in range(0,x.shape[0]):
  if y[i]!=1:
    ycoll[i][0]=0
  if y[i]!=2:
    ycoll[i][1]=0
  if y[i]==2:
    ycoll[i][1]=1
  if y[i]!=3:
    ycoll[i][2]=0
  if y[i]==3:
    ycoll[i][2]=1
  if y[i]!=4:
    ycoll[i][3]=0
  if y[i]==4:
    ycoll[i][3]=1
  if y[i]!=5:
    ycoll[i][4]=0
  if y[i]==5:
    ycoll[i][4]=1
  if y[i]!=6:
    ycoll[i][5]=0
  if y[i]==6:
    ycoll[i][5]=1
  if y[i]!=7:
    ycoll[i][6]=0
  if y[i]==7:
    ycoll[i][6]=1
  if y[i]!=8:
    ycoll[i][7]=0
  if y[i]==8:
    ycoll[i][7]=1
  if y[i]!=9:
    ycoll[i][8]=0
  if y[i]==9:
    ycoll[i][8]=1

#OBTAINING_DIFFERENT_THETA,COSTS(FOR ONE VS ALL CLASSIFICATION)
theta1,cost1=logistic_regression(x,ycoll[:,0],0.0001,200,100)
theta2,cost2=logistic_regression(x,ycoll[:,1],0.0001,200,100)
theta3,cost3=logistic_regression(x,ycoll[:,2],0.0001,200,100)
theta4,cost4=logistic_regression(x,ycoll[:,3],0.0001,200,100)
theta5,cost5=logistic_regression(x,ycoll[:,4],0.0001,200,100)
theta6,cost6=logistic_regression(x,ycoll[:,5],0.0001,200,100)
theta7,cost7=logistic_regression(x,ycoll[:,6],0.0001,200,100)
theta8,cost8=logistic_regression(x,ycoll[:,7],0.0001,200,100)
theta9,cost9=logistic_regression(x,ycoll[:,8],0.0001,200,100)

#IMPORTING TESTSET
dft=pd.read_csv("/content/sample_data/mnist_test.csv")
dft.head()

xt=(dft.drop(["7"],axis=1))/255 #NORMALISATION
yt=dft["7"]
print(yt)

n = xt.shape[1]
one_column = np.ones((xt.shape[0],1))
X = np.concatenate((one_column, xt), axis = 1)

#CALCULATING THE PARAMETERS REQUIRED FOR PROBABILITY MEASUREMENT
RHS1=hypothesisn(theta1,X,n) 
RHS2=hypothesisn(theta2,X,n)
RHS3=hypothesisn(theta3,X,n)
RHS4=hypothesisn(theta4,X,n)
RHS5=hypothesisn(theta5,X,n)
RHS6=hypothesisn(theta6,X,n)
RHS7=hypothesisn(theta7,X,n)
RHS8=hypothesisn(theta8,X,n)
RHS9=hypothesisn(theta9,X,n)

#BASE_PROBABILITY(P_ZERO)
Pzero=1/(1+np.exp(RHS1)+np.exp(RHS2)+np.exp(RHS3)+np.exp(RHS4)+np.exp(RHS5)+np.exp(RHS6)+np.exp(RHS7)+np.exp(RHS8)+np.exp(RHS9))

#OBTAINING OTHER PROBABILITIES USING BASE_PROBABILITY
P1=np.exp(RHS1)*Pzero
P2=np.exp(RHS2)*Pzero
P3=np.exp(RHS3)*Pzero
P4=np.exp(RHS4)*Pzero
P5=np.exp(RHS5)*Pzero
P6=np.exp(RHS6)*Pzero
P7=np.exp(RHS7)*Pzero
P8=np.exp(RHS8)*Pzero
P9=np.exp(RHS9)*Pzero

Pmat=np.vstack((P1,P2,P3,P4,P5,P6,P7,P8,P9))
Pmat=Pmat.transpose()

index=Pmat.argmax(axis=1)+1

maximum=np.max(Pmat,axis=1)



for i in range(X.shape[0]):
  if maximum[i]<0.09945:
    index[i]=0


count=0
for i in range(X.shape[0]):
  if index[i]==yt[i]:
    count+=1


accuracy=count/X.shape[0]*100

print(accuracy)
